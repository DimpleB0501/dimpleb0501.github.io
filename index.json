
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Biography I have a MSc in Biomedical engineering, BE in electronics engineering and over 9 years of research experience in the fields of robotics and computer vision. I am currently working as a Principal Engineer at TiH-IoT, IIT Bombay in the robotics group.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Biography I have a MSc in Biomedical engineering, BE in electronics engineering and over 9 years of research experience in the fields of robotics and computer vision. I am currently working as a Principal Engineer at TiH-IoT, IIT Bombay in the robotics group.","tags":null,"title":"Dimple Bhuta","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ccd9518b7564835dac05b15d9a8a0b0c","permalink":"https://dimpleb0501.github.io/research_projects/12_multispectral/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/12_multispectral/","section":"research_projects","summary":" Principal Engineer ‚Äì State Estimation and Control at Technology Innovation Hub for IoT and IoE (TIH-IoT), IIT-Bombay Currently leading two major initiatives - one focused on detecting underground pipeline leaks through advanced multispectral imaging, and another on drone-based onion crop phenotyping to power AI-driven farm advisory systems. ","tags":null,"title":"\u003cp align='justify'\u003e Multispectral Image Acquisition \u0026 Data Inference using UAV \u003c/p\u003e","type":"research_projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4dfb1192d59cb654236e41a3b3bee0e7","permalink":"https://dimpleb0501.github.io/research_projects/11_obs_avoid/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/11_obs_avoid/","section":"research_projects","summary":" Principal Engineer ‚Äì State Estimation and Control at Technology Innovation Hub for IoT and IoE (TIH-IoT), IIT-Bombay Reactive, Plug-and-Play Sense-and-Avoid System for Pixhawk-Based UAVs in Precision Agriculture ","tags":null,"title":"\u003cp align='justify'\u003e Lightweight Smart Sense-and-Avoid Module for Low-Altitude Agricultural UAVs\u003c/p\u003e","type":"research_projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6f6a37f2b3ecca80231ba4af5baf072c","permalink":"https://dimpleb0501.github.io/extra_project/6_leader_follower/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/extra_project/6_leader_follower/","section":"extra_project","summary":" In progress. ","tags":["robotics"],"title":"\u003cp align='justify'\u003e Coordinated Multi-Robot Navigation using ROS2 \u003c/p\u003e","type":"extra_project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fa9bdabba70525c734f58c12c4373c30","permalink":"https://dimpleb0501.github.io/research_projects/10_gnd_bot/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/10_gnd_bot/","section":"research_projects","summary":" Senior Engineer ‚Äì State Estimation and Control at Technology Innovation Hub for IoT and IoE (TIH-IoT), IIT-Bombay Sensors-based mobile robot localization and obstacle avoidance. ","tags":null,"title":"\u003cp align='justify'\u003e Encoder-Orientation Sensor Fusion for Ground Robot Navigation \u003c/p\u003e","type":"research_projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"adad5f8f82e59dd0f7abeda7d87baee5","permalink":"https://dimpleb0501.github.io/extra_project/5_wps_following_2wd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/extra_project/5_wps_following_2wd/","section":"extra_project","summary":" In progress. ","tags":["robotics"],"title":"\u003cp align='justify'\u003e Waypoint following for a drive differential drive robot \u003c/p\u003e","type":"extra_project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5964eb5fc2e53a30e58f24c093d02759","permalink":"https://dimpleb0501.github.io/research_projects/9_flvis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/9_flvis/","section":"research_projects","summary":" Senior Engineer ‚Äì State Estimation and Control at Technology Innovation Hub for IoT and IoE (TIH-IoT), IIT-Bombay Implementation of localization algorithm for precision agriculture. ","tags":null,"title":"\u003cp align='justify'\u003e Aerial Vehicle Localization Using a Downward-Facing Depth Camera for Precision Agriculture \u003c/p\u003e","type":"research_projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"86dc476b56b660f5b82dd2e357c3d1f0","permalink":"https://dimpleb0501.github.io/research_projects/1_cars_proj/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/1_cars_proj/","section":"research_projects","summary":" Senior research fellow at ARMS Lab, IIT-Bombay Developed reactive path planning and patrolling of a team of car like robots in campus like enviornment. ","tags":null,"title":"\u003cp align='justify'\u003e Path Planning and Patrolling for a team of Car-like Robots in a Campus Environment \u003c/p\u003e","type":"research_projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d62e525aa5ac6846a321e7b57dc9470d","permalink":"https://dimpleb0501.github.io/research_projects/2_obj_det_track/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/2_obj_det_track/","section":"research_projects","summary":" Software Developer at AITOE Lab Developed object detection and tracking algorithms. ","tags":null,"title":"\u003cp align='justify'\u003e Object detection and tracking \u003c/p\u003e","type":"research_projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7a6192f8e0cc93366691f8b2b2d05073","permalink":"https://dimpleb0501.github.io/research_projects/3_prog_manipulator/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/3_prog_manipulator/","section":"research_projects","summary":" Drone and Robotics Engineer at Dhristi works Worked on implementation of motion planning algorithms for a manipulator with an intended application of beach cleaning. ","tags":null,"title":"\u003cp align='justify'\u003e Programming robotic manipulator for pick and place task \u003c/p\u003e","type":"research_projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"367658bd78f359eb377fb0535cf14c2f","permalink":"https://dimpleb0501.github.io/research_projects/5_slip_control/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/5_slip_control/","section":"research_projects","summary":" Research assistant at Singapore Institute of Neurotechnology, National University of Singapore Designed slip control experimental setup with WidowX robot (under the guidance of a post-doctoral fellow), to emulate human reflexes in case of slip conditions. ","tags":null,"title":"\u003cp align='justify'\u003e Slip control experimental setup design \u003c/p\u003e","type":"research_projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9782e790101cf701540b4fd1dd1ecfa6","permalink":"https://dimpleb0501.github.io/research_projects/6_tactile_haptic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/6_tactile_haptic/","section":"research_projects","summary":" Research assistant at Singapore Institute of Neurotechnology, National University of Singapore Developed a haptic glove and a graphical user interface, intending to render and replicate the sense of touch. Designed a first-generation interface between the tactile and the haptic glove to enable a user to feel the object gripped by the robot. ","tags":null,"title":"\u003cp align='justify'\u003e Tactile and haptic glove interface \u003c/p\u003e","type":"research_projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4ab517ae98eda84f129b4007ae82f22f","permalink":"https://dimpleb0501.github.io/research_projects/4_motion_robot/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/4_motion_robot/","section":"research_projects","summary":" Research assistant at Singapore Institute of Neurotechnology, National University of Singapore Developed motion planning algorithms for the Universal Robots (UR10) to perform day-to-day tasks such as picking up the coin, bread cutting and opening the corkscrew for wine bottle.\n","tags":null,"title":"\u003cp align='justify'\u003e Development of motion planning algorithms for robots \u003c/p\u003e","type":"research_projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d396e24571fcf22480394a6fae1394d1","permalink":"https://dimpleb0501.github.io/research_projects/7_jewell_segmentation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/7_jewell_segmentation/","section":"research_projects","summary":" Project Assistant at Multimodal Perception Laboratory, International Institute of Information Technology Compared and Implemented algorithms for segmentation of jewelry in images. ","tags":null,"title":"Jewellery segmentation","type":"research_projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"adbfe3e67833f0bb6c3062376b5819de","permalink":"https://dimpleb0501.github.io/research_projects/8_pattern_rec/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research_projects/8_pattern_rec/","section":"research_projects","summary":" Project Manager - Embedded Software Engineer at Infinite Biomedical Technologies Conceptualized and engineered a pattern recognition based prosthetic arm which successfully classified subject‚Äôs hand positions using EMG signals. The aim of this project was to assist trans radial amputees to control their prosthesis. ","tags":null,"title":"\u003cp align='justify'\u003ePattern recognition based prosthetic arm\u003c/p\u003e","type":"research_projects"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://dimpleb0501.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Dimple Bhuta","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"import libr print(\u0026#39;hello\u0026#39;) Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It‚Äôs a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy‚Äôs future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://dimpleb0501.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":null,"categories":["R"],"content":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA ) Figure 1: A fancy pie chart.\n","date":1606875194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606875194,"objectID":"84a876ba789bb7232be8d9ed2487fd98","permalink":"https://dimpleb0501.github.io/post/2020-12-01-r-rmarkdown/","publishdate":"2020-12-01T21:13:14-05:00","relpermalink":"/post/2020-12-01-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":null,"categories":null,"content":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you‚Äôll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;} - Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal ``` renders as\n- Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap - Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ ``` renders as\n- Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ Example inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$ f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases} $$ Diagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders ‚Ä¶","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://dimpleb0501.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"post"},{"authors":["Dimple Bhuta"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://dimpleb0501.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Dimple Bhuta"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post‚Äôs folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://dimpleb0501.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://dimpleb0501.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Dimple Bhuta","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://dimpleb0501.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Dimple Bhuta","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://dimpleb0501.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"329ab89006974bac8b583c137bb95787","permalink":"https://dimpleb0501.github.io/extra_project/1_behavioralcloning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/extra_project/1_behavioralcloning/","section":"extra_project","summary":" Self-Driving Car Udacity Nanodegree Program Successfully emulated human driving behavior autonomously in a simulated environment. ","tags":["demo"],"title":"\u003cp align='justify'\u003e Behavioral Cloning \u003c/p\u003e","type":"extra_project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4fd73e96bc67ab19c21570908709d920","permalink":"https://dimpleb0501.github.io/extra_project/3_facegen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/extra_project/3_facegen/","section":"extra_project","summary":" Deep Learning Udacity Nanodegree Program Implemented fake face generation using general adversarial networks (GANs). ","tags":["demo"],"title":"\u003cp align='justify'\u003e Face generation \u003c/p\u003e","type":"extra_project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8785eecceab0e026891a67945815569a","permalink":"https://dimpleb0501.github.io/extra_project/2_foodorder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/extra_project/2_foodorder/","section":"extra_project","summary":" Implemented a food ordering application. ","tags":["extra"],"title":"\u003cp align='justify'\u003e Food Ordering App \u003c/p\u003e","type":"extra_project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"db0b83adebe837a6cbb6c9ca1e6f55e4","permalink":"https://dimpleb0501.github.io/extra_project/4_imagecapt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/extra_project/4_imagecapt/","section":"extra_project","summary":" Computer Vision Udacity Nanodegree Program Generated captions for the image using deep learning. ","tags":["demo"],"title":"\u003cp align='justify'\u003e Image captioning \u003c/p\u003e","type":"extra_project"},{"authors":null,"categories":null,"content":" Side-by-Side Figures with Links In this project at TIH, IIT-Bombay , we evaluated visual inertial odometry (VIO) systems for robot pose estimation based on Experimental Evaluation of VIO Systems for Arable Farming paper. In VIO systems, camera captures rich information of the scene at a low frame rate whilst IMU gets motion information at a high rate. ‚ÄÉ VIO systems allow robot pose estimation even in GNSS denied areas and are independent of the robot locomotion. However agricultural fields entail a particular challenge for VIO systems since, among their characteristics, they have changing lighting conditions, highly self-similar textures, and unstructured and dynamic objects. Furthermore, the irregular terrain of the field causes more aggressive camera motion than what is usually seen in urban and indoor cases. ‚ÄÉ We simulated an agricultural field in a gazebo environment with crop rows. And estimated the iris drone\u0026#39;s (with integrated intel realsense depth camera) position and orientation using feedback based visual inertial system (FVIS) algorithm. The following video displays our implementation. Figure: Localization of iris quadcopter in agricultural field ‚ÄÉ To test the implementation, we ran experiments in three Gazebo-simulated environments: A Manhattan word with rich and diverse features A structured farm with a regular grid arrangement An unstructured, randomly generated farm The figure below illustrates each simulation world and provides a link to its corresponding YouTube demonstration. Manhattan World Farm grid world Farm random arrangement The plots compare the drone‚Äôs true flight path and velocity profile obtained from onboard odometry with the trajectory and speed estimates generated by VIO-SLAM implementation for different heights (3.5 meter, 10 meter and 20 meter). Manhattan world: position plots at different heights Manhattan world: velocity plots at different heights Farm grid world: position plots at different heights Farm grid world: velocity plots at different heights Farm random world: position plots at different heights Farm random world: velocity plots at different heights ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f04c1066cd7fdb480a0740d0ed86880f","permalink":"https://dimpleb0501.github.io/project/9_flvis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/9_flvis/","section":"project","summary":"Side-by-Side Figures with Links In this project at TIH, IIT-Bombay , we evaluated visual inertial odometry (VIO) systems for robot pose estimation based on Experimental Evaluation of VIO Systems for Arable Farming paper.","tags":null,"title":"Aerial Vehicle Localization Using a Downward-Facing Depth Camera for Precision Agriculture","type":"project"},{"authors":null,"categories":null,"content":" I enrolled in a one month long mobile app development course conducted by Robotech labs organized by IIT, Bombay. ‚ÄÉ Following is the GitHub link for the apps that I developed during the course. Figure below displays snippets of food ordering and payment application that I successfully developed during the course. Figure: Food ordering and payment app ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5669c6b567467db59ae7becee0aaaf89","permalink":"https://dimpleb0501.github.io/project/b_foodordering/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/b_foodordering/","section":"project","summary":"I enrolled in a one month long mobile app development course conducted by Robotech labs organized by IIT, Bombay. ‚ÄÉ Following is the GitHub link for the apps that I developed during the course.","tags":null,"title":"Android development - Food ordering app","type":"project"},{"authors":null,"categories":null,"content":" I was selected for term 1 of self-driving cars nanodegree through KPIT scholarship. In the nanodegree, we implemented several projects like lane line detection, traffic signal classifier, behavioral cloning, and sensor fusion via extended kalman filters. Following is the GitHub link for the codes that I implemented during the course. ‚ÄÉ Of particular interest for me was the behavioral cloning project. In this project, our goal was to train a network using Keras which can successfully emulate human driving behavior autonomously in a simulated environment. I built a modified CNN based on NVIDIA‚Äôs End to End Learning for Self-Driving Cars paper. Figure: Car driving autonomously in the simulator ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2f51f9081595721c2ef19ef8966c9a09","permalink":"https://dimpleb0501.github.io/project/a_behaviorclone/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/a_behaviorclone/","section":"project","summary":"I was selected for term 1 of self-driving cars nanodegree through KPIT scholarship. In the nanodegree, we implemented several projects like lane line detection, traffic signal classifier, behavioral cloning, and sensor fusion via extended kalman filters.","tags":null,"title":"Behavioral Cloning","type":"project"},{"authors":null,"categories":null,"content":"In progress\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"646d782963e74e7d5af27b0568a67cc1","permalink":"https://dimpleb0501.github.io/project/f_lf/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/f_lf/","section":"project","summary":"In progress","tags":null,"title":"Coordinated Multi-Robot Navigation using ROS2","type":"project"},{"authors":null,"categories":null,"content":" (a) Bread cutting task (b) Corkscrew unlocking task (c) Programmatic controlling of a robotic hand ‚ÄÉ This part of my work was affiliated with the office of naval research Singapore (abbreviated ONR). Here I worked with a group of research scientists to make robots perform day-to-day tasks such as bread cutting and opening the corkscrew for a wine bottle. My work consisted of motion planning and control of the robots. I programmed the robot to perform zig-zag and spiral motions, as well as build an interface to control the digits of a robotic hand. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b86e9c66ede36a2d334228f5387ae8e7","permalink":"https://dimpleb0501.github.io/project/4_robot/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/4_robot/","section":"project","summary":"(a) Bread cutting task (b) Corkscrew unlocking task (c) Programmatic controlling of a robotic hand ‚ÄÉ This part of my work was affiliated with the office of naval research Singapore (abbreviated ONR).","tags":null,"title":"Development of motion planning algorithms for robots","type":"project"},{"authors":null,"categories":null,"content":" To achieve autonomous navigation for ground robot, I was involved in the development of, Encoder-based low-level controller developed with Raspberry Pi Pico, Sensor fusion for robot localization using encoders and orientation sensors, Velocity controller design for lidar-based obstacle avoidance using jetson orin nano. Figure: Autonomous navigation and avoidance for ground robot Figure 2: Obstacle avoidance with turtlebot The aim of this project was the development of a prototype rover that can be scaled up and used in the vineyard for data collection. Parallely we tested the velocity controller on turtlebot to validate obstacle avoidance performance. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85d540ad10887856dd32c19fd7d4acab","permalink":"https://dimpleb0501.github.io/project/10_gnd_bot/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/10_gnd_bot/","section":"project","summary":"To achieve autonomous navigation for ground robot, I was involved in the development of, Encoder-based low-level controller developed with Raspberry Pi Pico, Sensor fusion for robot localization using encoders and orientation sensors, Velocity controller design for lidar-based obstacle avoidance using jetson orin nano.","tags":null,"title":"Encoder-Orientation Sensor Fusion for Ground Robot Navigation","type":"project"},{"authors":null,"categories":null,"content":" Figure: Face generation with GANs I had applied to Bertelsmann Technology Scholarships and was among the Top 1,600 performers. As a result of which I was awarded a scholarship to Deep Learning Nanodegree program. A module of my program was dedicated to fake face generation using general adversarial networks (GANs). Figure above displays the generated samples (or generated faces) from my developed GAN model. ‚ÄÉ The generated samples do not exactly resemble human faces, but rather seem to be scary caricatures. This was because the training data consists of most images that are clipped below the chin area and were biased towards white celebrity faces. To improve the model work needs to be done towards creating a dataset comprising of high resolution images of complete faces of diverse ethnicity. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"21d35476a3d0414b2e714fecf97153a0","permalink":"https://dimpleb0501.github.io/project/c_facegen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/c_facegen/","section":"project","summary":"Figure: Face generation with GANs I had applied to Bertelsmann Technology Scholarships and was among the Top 1,600 performers. As a result of which I was awarded a scholarship to Deep Learning Nanodegree program.","tags":null,"title":"Face generation","type":"project"},{"authors":null,"categories":null,"content":"Image Captioning is the process of generating a textual description of an image. Basically, image captioning is a computer describing an input image in a sentence or paragraph. It uses concepts of Computer Vision and Natural Language Processing fields to generate the captions. Figure below displays my developed model\u0026#39;s predictions on sample images.\nFigure: Image with correct prediction Figure: Image where model could have performed better] ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"93c344f42cf8f8bbeddd523d69c21e7a","permalink":"https://dimpleb0501.github.io/project/d_capt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/d_capt/","section":"project","summary":"Image Captioning is the process of generating a textual description of an image. Basically, image captioning is a computer describing an input image in a sentence or paragraph. It uses concepts of Computer Vision and Natural Language Processing fields to generate the captions.","tags":null,"title":"Image captioning","type":"project"},{"authors":null,"categories":null,"content":" In digital image processing and computer vision, image segmentation is a process of partitioning a digital image into multiple segments. It helps to simplify the image as well as the partitioned image is eventually assigned labels which with further processing play an important role in object detection within an image. ‚ÄÉ I worked as a Project Assistant at Multimodal Perception Laboratory, International Institute of Information Technology, Bangalore, India. My work here was to analyze different image segmentation algorithms and develop an algorithm that would segment the background in shared jewelry images. The output of our algorithm was to replace the original background with transparent background. Our vendors intended to replace the background, edit the image and use it on selling websites. ‚ÄÉ Figures 1 and 2 displays our algorithm‚Äôs performance on original images shared by our vendors. Figure 1: Segmentation algorithm (best case) Figure 2: Segmentation algorithm (worst case) ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"17197be603ab0965410e80afabf8cd7c","permalink":"https://dimpleb0501.github.io/project/7_segmentation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/7_segmentation/","section":"project","summary":"In digital image processing and computer vision, image segmentation is a process of partitioning a digital image into multiple segments. It helps to simplify the image as well as the partitioned image is eventually assigned labels which with further processing play an important role in object detection within an image.","tags":null,"title":"Jewellery segmentation","type":"project"},{"authors":null,"categories":null,"content":" Abstract This paper introduces a lightweight framework designed to enhance UAV operations for low-altitude flying in agriculture fields. Farmlands are often filled with obstacles such as trees, structures, and poles, which can be dangerous when flying low. To address this challenge, the proposed system combines depth data from RGB-D camera with an IMU to generate LiDAR-like data, ensuring seamless integration with any range sensor-based avoidance algorithms. The entire perception and processing stack runs on the sense-and-avoid module, which can be easily mounted on MAVLink compatible flight controller. This allows the UAV to detect and avoid obstacles in real time, transitioning from semi-autonomous to fully autonomous flight control. By minimizing yaw during navigation, the system ensures both energy efficiency and stability, using only 40 % of the onboard processor, leaving room for the integration of additional functionalities, such as aerial spraying, data collection, etc. The effectiveness of the system was validated through simulations and real-world tests, showing reliable performance in diverse agricultural settings. This vehicle-agnostic sense-and-avoid technology has broad applications in agriculture, surveillance, and disaster relief, improving UAV navigation in obstacle-dense environments. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7526583c48020386dca2a716c78ae444","permalink":"https://dimpleb0501.github.io/project/11_sna/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/11_sna/","section":"project","summary":"Development of reactive sense and avoidance tech for UAVs.","tags":null,"title":"Lightweight Smart Sense-and-Avoid Module for Low-Altitude Agricultural UAVs","type":"project"},{"authors":null,"categories":null,"content":"\u0026lt;!DOCTYPE html\u0026gt; Drones equipped with multispectral cameras offer a powerful, non-invasive method for capturing high-resolution spectral data across large and inaccessible areas. These systems enable early detection of surface anomalies and plant stress by analyzing reflectance patterns beyond the visible spectrum. I am currently leading a team focused on multispectral data acquisition and AI-based inferencing for two critical projects: Surface Detection of Oil and Water from Underground Pipelines ‚Äì Utilizing spectral analysis to detect oil leaks and water contamination on the surface, supporting early intervention and environmental protection. AI-Based Farm Advisory System for Onion Cultivation ‚Äì Applying drone phenotyping to monitor plant health, detect stress conditions like thrips and anthracnose, and provide yield predictions to assist farmers via the iSaarthi mobile platform. These projects aim to solve real-world challenges in infrastructure safety and precision agriculture using drone-enabled multispectral intelligence. Surface Detection of Oil and Water from Underground Pipelines This project, in collaboration with HPCL, IOCL, and GAIL, focuses on detecting surface signatures of oil and water leaks originating from underground pipelines using multispectral imaging and AI-based analysis. Problem Statement: Pilferage Detection A significant challenge faced by oil and gas companies is the undetected pilferage and leakage of oil from underground pipelines. Early surface-level detection can prevent environmental damage, operational downtime, and economic losses. Figure: Multispectral image analysis for oil vs water pipeline Preliminary Spectral Analysis Initial analysis focused on identifying spectral bands that effectively distinguish between oil and water on soil and vegetation. Multispectral imaging helped isolate key wavelengths where oil and water show distinct reflectance patterns. Figure: Spectral band comparison of oil and water Dataset Creation and Model Development A custom multispectral dataset was created under controlled conditions using varying quantities of oil, water, and vegetation. This comprehensive dataset enabled the training of a robust deep learning model based on the YOLOv8 architecture. The model achieved an accuracy of 92.9% for oil detection and 96% for water detection, showcasing strong performance in surface leak classification tasks. Phase 2: Real-World Deployment In the second phase of the project, we plan to augment the current dataset with real-world data collected from actual pipeline leakage sites, in collaboration with industry partners such as IOCL, HPCL, and GAIL. This phase aims to validate and improve the model\u0026#39;s performance under diverse environmental and terrain conditions. Field data will include site-specific spectral signatures, soil types, and contamination patterns to ensure the model generalizes well to operational settings. In parallel, comparative benchmarking against other state-of-the-art deep learning algorithms is ongoing, and the results will contribute to a scientific publication currently under development. AI-Based Farm Advisory System for Onion Cultivation In collaboration with ICAR-DOGR (Indian Council of Agricultural Research - Directorate of Onion and Garlic Research), this project focuses on developing an AI-powered farm advisory system tailored for onion farming. Leveraging multispectral imaging and advanced machine learning models, the system aims to detect various forms of plant stress including thrips infestation, anthracnose disease, and water drought conditions. It also incorporates yield prediction models for improved crop management. The ultimate goal is to integrate drone-based phenotyping with the iSaarthi mobile application, which aggregates data from ground control stations, onboard agro-sensors, and mobile imagery captured by farmers. This fusion of aerial and ground-level data supports real-time, location-specific advisories to empower farmers with actionable insights. Figure: AI-based farm advisory system pipeline ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3165610ac0bc9a498f09f22af3213c20","permalink":"https://dimpleb0501.github.io/project/12_spectral/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/12_spectral/","section":"project","summary":"\u003c!DOCTYPE html\u003e Drones equipped with multispectral cameras offer a powerful, non-invasive method for capturing high-resolution spectral data across large and inaccessible areas. These systems enable early detection of surface anomalies and plant stress by analyzing reflectance patterns beyond the visible spectrum.","tags":null,"title":"Multispectral Image Acquisition \u0026 Data Inference using UAVs","type":"project"},{"authors":null,"categories":null,"content":" At AitoeLabs, I worked with a team of engineers to build efficient video analytic solutions for internal security. To ensure ATM security we developed deep learning models and algorithmic codes on the live video stream. ‚ÄÉ I was affiliated with the object detection and tracking group. Our implemented human detection algorithm‚Äôs performance is displayed in below.\nFigure: Detecting multiple objects (people) in a live stream ‚ÄÉ Object tracking deals with tracking detected objects by assigning IDs to them. A theft or vandalizing of an ATM generally happens when there is a group of people (more than the allowed number) in the ATM or if a person is loitering around inside the ATM for a prolonged duration. Using object tracking methodology we developed algorithms to detect this kind of behavior and on detection alerted the security personnel/ company.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3aee062131ff44eb0bf1408a32310e4a","permalink":"https://dimpleb0501.github.io/project/2_objtrack/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/2_objtrack/","section":"project","summary":"At AitoeLabs, I worked with a team of engineers to build efficient video analytic solutions for internal security. To ensure ATM security we developed deep learning models and algorithmic codes on the live video stream.","tags":null,"title":"Object detection and tracking","type":"project"},{"authors":null,"categories":null,"content":" For self-driving cars to traverse roads, it requires a robust path planning system. The planning system is responsible for high-level path planning such as route planning (which roads/ route to take to reach from point A to point B), path generation (generating a smooth curve to avoid abrupt acceleration/ deceleration and jerky behavior of car), and low-level path planning such as behavior selection (selecting behavior such as lane-keeping, intersection handling, traffic light handling), motion planning (generating the reference path along with reference speeds) and obstacle avoidance (reactive avoiding obstacles in its path). ‚ÄÉ During my work at Indian Institute of Technology, Bombay, I have worked towards designing and developing several modules of a reactive path planning system for autonomous mobile vehicles, allowing the vehicles to navigate predefined areas from the world map. Figure below displays multiple vehicles traversing roads in our simulated environment. Figure: Cars traversing roads in an area selected in the world map ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9dc0e0624d46585136342b0cbe7a1e21","permalink":"https://dimpleb0501.github.io/project/1_cars/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/1_cars/","section":"project","summary":"For self-driving cars to traverse roads, it requires a robust path planning system. The planning system is responsible for high-level path planning such as route planning (which roads/ route to take to reach from point A to point B), path generation (generating a smooth curve to avoid abrupt acceleration/ deceleration and jerky behavior of car), and low-level path planning such as behavior selection (selecting behavior such as lane-keeping, intersection handling, traffic light handling), motion planning (generating the reference path along with reference speeds) and obstacle avoidance (reactive avoiding obstacles in its path).","tags":null,"title":"Path Planning and Patrolling for a team of Car-like Robots in a Campus Environment","type":"project"},{"authors":null,"categories":null,"content":" The following section entails my work at Infinite biomedical technologies (a John Hopkins-affiliated research lab in the USA). Here I worked as a project manager, and with my team developed a stand-alone device that non-invasively records 8-channels of the subject‚Äôs EMG signals (in real-time) to differentiate, determine and control prosthetic hand positions. We successfully implemented hand-open, close, flex, extend, pronate, supinate and hook hand positions. ‚ÄÉ The images below (as well as the YouTube video) display the working of our interface for hand open and close positions. The EMG patterns generated when the subject opens or closes their hand is analyzed by our device and based on the classification result the robotic hand mimics the subject‚Äôs hand position. Figure: Controlling prosthetic hand based on subject‚Äôs EMG patterns ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2b4bd3fea37df0a3e8e7457be62f916b","permalink":"https://dimpleb0501.github.io/project/8_patternrec/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/8_patternrec/","section":"project","summary":"The following section entails my work at Infinite biomedical technologies (a John Hopkins-affiliated research lab in the USA). Here I worked as a project manager, and with my team developed a stand-alone device that non-invasively records 8-channels of the subject‚Äôs EMG signals (in real-time) to differentiate, determine and control prosthetic hand positions.","tags":null,"title":"Pattern recognition based prosthetic arm","type":"project"},{"authors":null,"categories":null,"content":" At my job at Dhristi works, we aimed to develop garbage bots for beach cleaning. My focus was on developing algorithms for a robotic arm for pick and place tasks. The application would be used to identify and pick garbage on the beach and place it on the bag/bin on top of a mobile rover (which was parallelly being developed by other team members). ‚ÄÉ I used ROS to set up an interface between the actual robot(purchased 5DOF robotic arm) and gazebo simulator. Thus controlling the robot both in the simulated and real environment. The Figure below displays the robot being controlled by ROS in both real and simulated environments. Figure 1: Robot mimicking position via commands from ROS Figure 2: Robotic arm grasping household objects To grasp household objects, I compared and analyzed several available grasping techniques. And interfaced the selected technique with our simulated robot. Figure 2 displays the scene creation in a simulated environment consisting of robot and household objects. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"32cbf4e848d4a29f14f1e76848448609","permalink":"https://dimpleb0501.github.io/project/3_manipulator/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/3_manipulator/","section":"project","summary":"At my job at Dhristi works, we aimed to develop garbage bots for beach cleaning. My focus was on developing algorithms for a robotic arm for pick and place tasks.","tags":null,"title":"Programming robotic manipulator for pick and place task","type":"project"},{"authors":null,"categories":null,"content":" Figure: Slip control experimental setup To study the occurrence of slippage when mass is added to an object gripped by a robotic end-effector, I designed an experimental setup to recreate slip conditions and test friction models developed by the post-doctoral fellow. ‚ÄÉ Slip conditions can be interpreted by normal and tangential forces of the object on the end- effector. I worked on developing a high-density tactile sensor array to fit the surface area of WidowX‚Äôs (purchased robot) parallel gripper, as well as built circuitry and designed algorithms to read tactile, force, and optical sensor data every 0.01 second using Simulink software. This part of the setup would provide the magnitude and direction of the normal force and would serve as input to the friction model/ controller designed by the post-doctoral fellow. ‚ÄÉTo control the orientation of the robot and adjust the gripper position, I developed an interface which permitted the gripper to perform micromotions every 0.01 seconds. The high-speed micromotions enables the WidowX to promptly control the grip of the robot once the slip event is detected by the friction model/ controller. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"09ff2e900b1aed31b9c56068475768c7","permalink":"https://dimpleb0501.github.io/project/5_slipcontrol/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/5_slipcontrol/","section":"project","summary":"Figure: Slip control experimental setup To study the occurrence of slippage when mass is added to an object gripped by a robotic end-effector, I designed an experimental setup to recreate slip conditions and test friction models developed by the post-doctoral fellow.","tags":null,"title":"Slip control experimental setup design","type":"project"},{"authors":null,"categories":null,"content":" This interface is a telerobotics application and consists of the following 3 parts: Development of tactile glove: We developed a fabric tactile sensor array in the form of a glove that would be fitted to a robotic hand. The output of the tactile sensor is inversely proportional to the pressure applied by an external object. Development of haptic glove: To facilitate the remote human operator to perceive and understand distant and inaccessible environments, we developed haptic glove intending to render and replicate the sense of touch. Interface: Worked on developing a first-generation interface that enables the user to feel the object gripped by the robot, by determining the amount of pressure applied to the sensorized tactile glove and equivalently modulating vibratory haptic feedback on the haptic glove worn by the user. ‚ÄÉ Depending on the amount of pressure applied to the sensorized robotic hand equivalent vibration will be be experienced by the user. Figure below displays that high pressure is applied to the robotic hand and equivalent intensity vibration will be felt by the remote user wearing the haptic glove. Figure: Tactile and haptic glove interface ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d48d23da86018db12a205cc1e69c278c","permalink":"https://dimpleb0501.github.io/project/6_haptic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/6_haptic/","section":"project","summary":"This interface is a telerobotics application and consists of the following 3 parts: Development of tactile glove: We developed a fabric tactile sensor array in the form of a glove that would be fitted to a robotic hand.","tags":null,"title":"Tactile and haptic glove interface","type":"project"},{"authors":null,"categories":null,"content":"In progress\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0d35196b7990c913e37c2471bbd2ad66","permalink":"https://dimpleb0501.github.io/project/e_2wdd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/e_2wdd/","section":"project","summary":"In progress","tags":null,"title":"Waypoint following for 2 wheel drive differential drive robot","type":"project"}]